{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ongoing-value",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-a29cc613415a>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-a29cc613415a>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    tf_upgrade_v2 --infile tf1totf2.py --outfile tf1totf2-upgraded.py\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "tf_upgrade_v2 --infile tf1totf2.py --outfile tf1totf2-upgraded.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from gen_meta_omni import Gen_data_Meta\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.contrib.layers.python import layers as tf_layers\n",
    "## Error not show!!\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "data_folder='data/omniglot/data/'\n",
    "num_in=5 # K of K-shot N-Way\n",
    "num_in_and_out=50 # total number of data in D_{test}\n",
    "dim_hidden=[64,64,64,64]\n",
    "num_batch=4 # meta-batch size\n",
    "num_fake_img = 3  # number of fake image (M)\n",
    "\n",
    "meta_lr=1e-3 # meta-traing learning rate (gamma)\n",
    "update_lr=1e-1 # learning rate for inner gradient update (alpha)\n",
    "\n",
    "# 전체 Data에서 k=1000개는 training, 그 외는 test dataset으로 분할\n",
    "gen_class=Gen_data_Meta(data_folder=data_folder,num_in=num_in,mode='train',\n",
    "                            num_in_and_out=num_in_and_out,k=1000)\n",
    "# # 전체 1623개 중 1000 + 623으로 분할됨\n",
    "# print(len(gen_class.character_folders))\n",
    "# print(len(gen_class.character_folders_train))\n",
    "# print(len(gen_class.character_folders_test))\n",
    "\n",
    "X_in=tf.placeholder('float',[num_batch,num_in, 28,28,1])\n",
    "X_in_and_out=tf.placeholder('float',[num_batch,num_in_and_out, 28,28,1])\n",
    "Y_in_and_out=tf.placeholder('float',[num_batch,num_in_and_out])\n",
    "\n",
    "def loss_softmax(input, par, input_fake=None, reuse=True, label=None):\n",
    "    repr=input\n",
    "    for i in range(len(dim_hidden)):\n",
    "        stride,no_stride=[1,2,2,1],[1,1,1,1]\n",
    "        conv_output=tf.nn.conv2d(repr,par['w'+str(i+1)],strides=stride,padding='SAME')+par['b'+str(i+1)]\n",
    "        repr = tf_layers.batch_norm(conv_output,activation_fn=tf.nn.elu,reuse=reuse,scope=str(i+1))\n",
    "    input_fc = tf.layers.flatten(repr)\n",
    "    input_loss = tf.matmul(input_fc, par['w_fc1']) + par['b_fc1']\n",
    "\n",
    "    if label == None:\n",
    "        input_shape = input.shape.as_list()\n",
    "        input_fake_shape = input_fake.shape.as_list()\n",
    "\n",
    "        repr_fake=tf.sigmoid(input_fake)\n",
    "        for i in range(len(dim_hidden)):\n",
    "            stride,no_stride=[1,2,2,1],[1,1,1,1]\n",
    "            conv_output=tf.nn.conv2d(repr_fake,par['w'+str(i+1)],strides=stride,padding='SAME')+par['b'+str(i+1)]\n",
    "            repr_fake = tf_layers.batch_norm(conv_output,activation_fn=tf.nn.elu,reuse=reuse,scope=str(i+1))\n",
    "        input_fc = tf.layers.flatten(repr_fake)\n",
    "        fake_loss = tf.matmul(input_fc, par['w_fc1']) + par['b_fc1']\n",
    "        \n",
    "        label_fake = tf.concat((tf.ones([input_fake_shape[0], 1]), tf.zeros([input_fake_shape[0], 1])), axis=1)\n",
    "        label_real = tf.concat((tf.zeros([input_shape[0], 1]), tf.ones([input_shape[0], 1])), axis=1)\n",
    "\n",
    "        result_img = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=label_real, logits=input_loss))\n",
    "        result_fake = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=label_fake, logits=fake_loss))\n",
    "        result = result_img + result_fake\n",
    "    else:\n",
    "\n",
    "        label_re = tf.reshape(label, [-1, 1])\n",
    "        label_real = tf.concat((label_re, 1.0 - label_re), axis=1)\n",
    "        result = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=label_real, logits=input_loss))\n",
    "\n",
    "    return result\n",
    "\n",
    "def loss_adv(input_fake, par):\n",
    "    repr_fake=tf.sigmoid(input_fake)\n",
    "    for i in range(len(dim_hidden)):\n",
    "        stride,no_stride=[1,2,2,1],[1,1,1,1]\n",
    "        conv_output=tf.nn.conv2d(repr_fake,par['w'+str(i+1)],strides=stride,padding='SAME')+par['b'+str(i+1)]\n",
    "        repr_fake = tf_layers.batch_norm(conv_output,activation_fn=tf.nn.elu,reuse=True,scope=str(i+1))\n",
    "    input_fc = tf.layers.flatten(repr_fake)\n",
    "    input_fake = tf.matmul(input_fc, par['w_fc1']) + par['b_fc1']\n",
    "    logit_fake = tf.concat((tf.zeros([tf.shape(input_fake)[0], 1]), tf.ones([tf.shape(input_fake)[0], 1])), axis=1)\n",
    "\n",
    "    result_fake = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=logit_fake, logits=input_fake))\n",
    "\n",
    "    return result_fake\n",
    "\n",
    "\n",
    "def Task_learn(inp):\n",
    "    X_i, X_target2_i, Y_target2_i = inp\n",
    "    num_iter=1\n",
    "    loss_real = loss_softmax(input=X_i, input_fake=fake_img, par=par_real1, reuse=True) # L_{\\theta}(D_{train}, \\theta_{fake})\n",
    "    grad_real = tf.gradients(loss_real, list(par_real1.values())) # \\partial(L) / \\partial(\\theta)\n",
    "    gradient_real = dict(zip(par_real1.keys(), grad_real)) # dict(zip([a1,a2,..],[b1,b2,..]))는 'a1':b1, 'a2':b2..로 dictionary를 연결해준다. \n",
    "    # Eq. (1)\n",
    "    par_real_update1 = dict(zip(par_real1.keys(), [par_real1[key] - update_lr * gradient_real[key] for key in par_real1.keys()]))\n",
    "\n",
    "    for k in range(num_iter-1):\n",
    "        loss_real = loss_softmax(input=X_i, input_fake=fake_img, par=par_real_update1, reuse=True) # same with above\n",
    "        grad_real = tf.gradients(loss_real, list(par_real_update1.values())) # same with above\n",
    "        gradient_real = dict(zip(par_real_update1.keys(), grad_real)) # same with above\n",
    "        # Eq. (1)\n",
    "        par_real_update1 = dict(zip(par_real_update1.keys(), [par_real_update1[key] - update_lr * gradient_real[key] for key in par_real_update1.keys()]))\n",
    "    \n",
    "    loss_real_adv = loss_adv(input_fake=fake_img,  par=par_real_update1) # L_{\\theta}(\\theta_{fake})\n",
    "    grad_real_adv = tf.gradients(loss_real_adv, [fake_img]) # \\partial(L) / \\partial(\\theta_{fake})\n",
    "    grad_real_adv = tf.stop_gradient(grad_real_adv) \n",
    "    # Eq. (2)\n",
    "    input_fake_update = fake_img - tf.multiply(tf.nn.softplus(beta_fake) , tf.sign(grad_real_adv[0])) # \\theta_{fake} - \\beta_{fake}\\odot sign(grad)\n",
    "    # softplus : log(exp(x) + 1)  => 음수가 나오지 않도록 넣은듯 => 음수일때는 0\n",
    "    # multiply : point wise product\n",
    "\n",
    "    for k in range(2):\n",
    "        loss_real_adv = loss_adv(input_fake=input_fake_update, par=par_real_update1) # update된 input_fake 입력\n",
    "        grad_real_adv = tf.gradients(loss_real_adv, [input_fake_update])\n",
    "        grad_real_adv = tf.stop_gradient(grad_real_adv) #괄호 내의 파란미터는 학습되지 않는다.\n",
    "        # Eq. (2)\n",
    "        input_fake_update = input_fake_update - tf.multiply(tf.nn.softplus(beta_fake) , tf.sign(grad_real_adv[0]))\n",
    "\n",
    "    loss_adv_final = loss_adv(input_fake=input_fake_update,par=par_real_update1)\n",
    "    fake_stack = tf.concat((fake_img, input_fake_update), axis=0)\n",
    "    \n",
    "    loss_real = loss_softmax(input=X_i, input_fake=fake_stack, par=par_real1, reuse=True)\n",
    "    grad_real = tf.gradients(loss_real, list(par_real1.values()))\n",
    "    gradient_real = dict(zip(par_real1.keys(), grad_real))\n",
    "    # Eq. (3)\n",
    "    par_real_update = dict(zip(par_real1.keys(), [par_real1[key] - update_lr * gradient_real[key] for key in par_real1.keys()]))\n",
    "    \n",
    "    num_iter=3\n",
    "    for k in range(num_iter-1):\n",
    "        loss_real = loss_softmax(input=X_i, input_fake=fake_stack, par=par_real_update, reuse=True)\n",
    "        grad_real = tf.gradients(loss_real, list(par_real_update.values()))\n",
    "        gradient_real = dict(zip(par_real_update.keys(), grad_real))\n",
    "        # Eq. (3)\n",
    "        par_real_update = dict(zip(par_real_update.keys(),[par_real_update[key] - update_lr * gradient_real[key] for key in par_real_update.keys()]))\n",
    "    \n",
    "    # Eq. (4)\n",
    "    loss_real_tar = loss_softmax(input=X_target2_i, par=par_real_update, label=Y_target2_i, reuse=True)\n",
    "            #     def loss_softmax(input,             par,  input_fake=None, reuse=True, label=None):\n",
    "\n",
    "    return loss_real_tar\n",
    "\n",
    "# def construct_conv_weights():\n",
    "initializer_w=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "initializer_b=tf.contrib.layers.xavier_initializer()\n",
    "par_real1={}\n",
    "\n",
    "with tf.variable_scope(\"par_real\",reuse=tf.AUTO_REUSE):\n",
    "    shape_w=[3,3,1,dim_hidden[0]]  # 필터 64개?\n",
    "    shape_b=[dim_hidden[0]]        # 64개\n",
    "\n",
    "    par_real1['w1'] = tf.Variable(initializer_w(shape=shape_w),name='w1')\n",
    "    par_real1['b1'] = tf.Variable(initializer_b(shape=shape_b),name='b1')\n",
    "    for i in range(1, len(dim_hidden)): # i = 1,2,3\n",
    "        shape_w=[3,3,dim_hidden[i - 1], dim_hidden[i]]\n",
    "        shape_b=[dim_hidden[i]]\n",
    "\n",
    "        par_real1['w' + str(i + 1)] = tf.Variable(initializer_w(shape=shape_w), name='w' + str(i + 1))\n",
    "        par_real1['b' + str(i + 1)] = tf.Variable(initializer_b(shape=shape_b), name='b' + str(i + 1))\n",
    "\n",
    "with tf.variable_scope(\"par_fake\", reuse=tf.AUTO_REUSE):\n",
    "    initializer_fake = tf.random_normal_initializer()\n",
    "    initializer_alpha = tf.ones_initializer()\n",
    "    fake_img_shape = [num_fake_img, 28,28, 1]\n",
    "    fake_img_par = tf.Variable(initializer_fake(shape=fake_img_shape), name='fake_img')\n",
    "    beta_fake = tf.Variable(initializer_alpha(shape=fake_img_shape), name='alpha')\n",
    "\n",
    "fake_img=fake_img_par\n",
    "# Vaiable : par_real1 : [w1~w4, b1~b4]\n",
    "# Vaiable : fake_img(shape([num_fake_img=3, 28, 28, 1]))\n",
    "# Vaiable : beta_fake(shape([num_fake_img=3, 28, 28, 1])) = 1로 초기화\n",
    "# par_real1, fake_img=fake_img_par, beta_fake  # theta, theta_{fake}, beta_fake\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "with tf.variable_scope('par_real/', reuse=tf.AUTO_REUSE):\n",
    "    dim_w1 = [64*4, 2]\n",
    "    dim_b1 = [2]\n",
    "    initializer_fc=tf.contrib.layers.xavier_initializer()\n",
    "    # 파라미터 초기화\n",
    "    par_real1['w_fc1']=tf.Variable(name='w_fc1',initial_value=initializer_fc(shape=dim_w1))\n",
    "    par_real1['b_fc1']=tf.Variable(name='b_fc1',initial_value=initializer_fc(shape=dim_b1))\n",
    "\n",
    "# Vaiable : par_real1 : [w1~w4, b1~b4, w_fc1, b_fc1]\n",
    "\n",
    "unused=X_in[0]\n",
    "for i in range(len(dim_hidden)):\n",
    "    stride,no_stride=[1,2,2,1],[1,1,1,1]\n",
    "    conv_output=tf.nn.conv2d(unused,par_real1['w'+str(i+1)],strides=stride,padding='SAME')+par_real1['b'+str(i+1)]\n",
    "    unused = tf_layers.batch_norm(conv_output,activation_fn=tf.nn.elu,reuse=False,scope=str(i+1))\n",
    "loss_class=tf.map_fn(Task_learn, \n",
    "                   elems=(X_in, X_in_and_out, Y_in_and_out), \n",
    "                   dtype=tf.float32, \n",
    "                   parallel_iterations=num_batch)\n",
    "loss_class_final=tf.reduce_mean(loss_class)\n",
    "#######################################################################################\n",
    "\n",
    "Train_op0=tf.train.AdamOptimizer(learning_rate=meta_lr).minimize(loss_class_final)\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "# init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
    "\n",
    "max_iteration = 10\n",
    "for iter in range(max_iteration):\n",
    "    for j in range(num_batch):\n",
    "        X_in_j, X_in_and_out_j, Y_in_and_out_j=gen_class.construction_unknown_single_train()\n",
    "        X_in_j = np.expand_dims(np.expand_dims(X_in_j, 0),-1)\n",
    "        X_in_and_out_j = np.expand_dims(np.expand_dims(X_in_and_out_j, 0),-1)\n",
    "        if j == 0:\n",
    "            X_in_feed = X_in_j\n",
    "            X_in_and_out_feed = X_in_and_out_j\n",
    "            Y_in_and_out_feed = Y_in_and_out_j\n",
    "        else:\n",
    "            # np.vstack 세로 결합\n",
    "            X_in_feed = np.vstack((X_in_feed, X_in_j))\n",
    "            X_in_and_out_feed = np.vstack((X_in_and_out_feed, X_in_and_out_j))\n",
    "            Y_in_and_out_feed = np.vstack((Y_in_and_out_feed, Y_in_and_out_j))\n",
    "    loss_r1,_= sess.run((loss_class_final,Train_op0),\n",
    "                        feed_dict={X_in        : X_in_feed, \n",
    "                                   X_in_and_out: X_in_and_out_feed,\n",
    "                                   Y_in_and_out: Y_in_and_out_feed})\n",
    "    if iter %10 ==0:\n",
    "        print(loss_r1,iter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
